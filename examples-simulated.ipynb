{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fa1f2-187e-4ce0-af95-31d6120977fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from numpy import linalg as la\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gMLV import *\n",
    "#import importlib\n",
    "# import gMLV_ML\n",
    "# import gMLV_sim\n",
    "#importlib.reload(gMLV_ML);\n",
    "#importlib.reload(gMLV_sim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a82c9-f85e-49db-979d-f23fae68f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plotting functions\n",
    "def plot_fit_gLV(yobs, y0, nsp, m_h, M_h):\n",
    "    # plot the fit\n",
    "    cols = [\"red\", \"green\", \"blue\", \"royalblue\",\"black\"]\n",
    "    #cols = [1,2,3,4,5]\n",
    "    yobs_pred = odeint(gLV, y0, times, args=(nsp, mu_h, M_h))\n",
    "    for i in range(nsp):\n",
    "        plt.plot(times, yobs[:, i], color=cols[i])\n",
    "        plt.plot(times, yobs_pred[:, i], '--', color=cols[i])\n",
    "\n",
    "def compare_params(mu_h, mu, M_h, M):\n",
    "    print(\"\\ninferred params:\")\n",
    "    print(\"mu_hat/mu:\")\n",
    "    print(np.array(mu_h))\n",
    "    print(np.array(mu))\n",
    "    print(\"\\nM_hat/M:\")\n",
    "    print(np.round(np.array(M_h),decimals=2))\n",
    "    print(\"\\n\",np.array(M) )\n",
    "\n",
    "    # plot the params\n",
    "    plt.figure(figsize=(6.4*2,4.8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.stem(np.arange(0,len(mu), dtype=\"int32\"), np.array(mu_h), markerfmt=\"D\")\n",
    "    plt.stem(np.arange(0,len(mu), dtype=\"int32\"), np.array(mu), markerfmt=\"X\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.stem(np.arange(0,nsp*nsp), np.array(M_h).flatten(), markerfmt=\"D\")\n",
    "    plt.stem(np.arange(0,nsp*nsp), np.array(M).flatten(), markerfmt=\"X\")\n",
    "    \n",
    "def compare_params_p(mu_h, mu, M_h, M, e_h, e):\n",
    "    print(\"\\ninferred params:\")\n",
    "    print(\"mu_hat/mu:\")\n",
    "    print(np.array(mu_h))\n",
    "    print(np.array(mu))\n",
    "    print(\"\\nM_hat/M:\")\n",
    "    print(np.round(np.array(M_h),decimals=2))\n",
    "    print(\"\\n\",np.array(M) )\n",
    "    print(\"e_hat/e:\")\n",
    "    print(np.array(e_h))\n",
    "    print(np.array(e))\n",
    "\n",
    "    # plot the params\n",
    "    plt.figure(figsize=(6.4*3,4.8))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.stem(np.arange(0,nsp, dtype=\"int32\"), np.array(mu_h), markerfmt=\"D\")\n",
    "    plt.stem(np.arange(0,nsp, dtype=\"int32\"), np.array(mu), markerfmt=\"X\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.stem(np.arange(0,nsp*nsp), np.array(M_h).flatten(), markerfmt=\"D\")\n",
    "    plt.stem(np.arange(0,nsp*nsp), np.array(M).flatten(), markerfmt=\"X\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.stem(np.arange(0,nsp), np.array(e_h), markerfmt=\"D\")\n",
    "    plt.stem(np.arange(0,nsp), np.array(e), markerfmt=\"X\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9ef3f-7097-401a-80ab-fe8e92ff7eba",
   "metadata": {},
   "source": [
    "## Simulate some time course data and perform ridge regression as in Stein et al. 2013\n",
    "I have coded up the Stein model and ridge regression without the perturbation term (Ridge1) and with a single perturbation (Ridge 2). <br>\n",
    "Ridge regression is designed to cause shrinkage to prevent overfitting. It isn't supposed to be used for variable\n",
    "selection. We should use Lasso for this, however I think we need to constrain parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17afad1f-293b-4f8b-bc99-f9dd3fed4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five species, single time course. In this example n >> p and it it is basically same as standard regression\n",
    "# We have to be careful as most of these gLV models are very weakly identifiable\n",
    "nsp = 5\n",
    "times = np.arange(0, 5, 0.1)\n",
    "nt = len(times)\n",
    "print(f\"nt: {nt}\")\n",
    "\n",
    "yobs, y0, mu, M = sim_gLV_5(times=times, y0=[10, 10, 10, 10, 10])\n",
    "    \n",
    "# add some gaussian noise\n",
    "yobs = yobs + np.random.normal(loc=0, scale=0.1, size=yobs.shape )\n",
    "plt.plot(times, yobs)\n",
    "\n",
    "# linearise\n",
    "X, F = linearize_time_course_16S(yobs,times)\n",
    "print(f\"n: {nsp*F.shape[0]}, p: {nsp + nsp*nsp}\")\n",
    "\n",
    "# get the best lambda/alpha values on a grid via cross validation\n",
    "a0, a1 = fit_alpha_Ridge1(X, F, nsp=nsp, n_a0=20, n_a1=20)\n",
    "\n",
    "# do final fit\n",
    "mu_h, M_h = do_final_fit_Ridge1(X, F, nsp, a0, a1)\n",
    "plot_fit_gLV(yobs,y0,nsp,mu_h,M_h)\n",
    "\n",
    "# this does the stem plots with orange crosses the actual parameters\n",
    "compare_params(mu_h, mu, M_h, M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828116e-9999-40e3-93de-58e8b42c4584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five species, lower number of time points, multiple time course\n",
    "nsp = 5\n",
    "nind = 3\n",
    "\n",
    "times = np.arange(0,5,1)\n",
    "nt = len(times)\n",
    "print(f\"nt: {nt}\")\n",
    "\n",
    "ryobs = []\n",
    "ry0 = []\n",
    "X = np.array([], dtype=np.double).reshape(0,nsp+1)\n",
    "F = np.array([], dtype=np.double).reshape(0,nsp)\n",
    "for i in range(nind):\n",
    "    mu = np.random.lognormal(0.01, 0.5, nsp)\n",
    "    yobs, y0, mu, M = sim_gLV_5(times=times, y0=np.random.uniform(low=10, high=50, size=5), mu=mu)\n",
    "    yobs = yobs + np.random.normal(loc=0, scale=0.1, size=yobs.shape )\n",
    "    ryobs.append(yobs)\n",
    "    ry0.append(y0)\n",
    "    Xs, Fs = linearize_time_course_16S(yobs,times)\n",
    "    #if i==0:\n",
    "    X = np.vstack([X, Xs])\n",
    "    F = np.vstack([F, Fs])\n",
    "\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"F: {F.shape}\")\n",
    "print(f\"n: {nsp*F.shape[0]}, p: {nsp + nsp*nsp}\")\n",
    "\n",
    "## get the best lambda/alpha values on a grid via cross validation\n",
    "a0, a1 = fit_alpha_Ridge1(X, F, nsp=nsp, n_a0=20, n_a1=20)\n",
    "\n",
    "mu_h, M_h = do_final_fit_Ridge1(X, F, nsp, a0=a0, a1=a1)\n",
    "#mu_h, M_h = gMLV_ML.do_final_fit_Ridge1(X, F, nsp, a0=1, a1=0.01)\n",
    "#mu_h, M_h = gMLV_ML.do_final_fit_Ridge1(X, F, nsp, a0=0, a1=0)\n",
    "\n",
    "# plot three example fits\n",
    "plt.figure(figsize=(6.4*3,4.8))\n",
    "plt.subplot(1, 3, 1); plot_fit_gLV(ryobs[0],ry0[0],nsp,mu_h,M_h)\n",
    "plt.subplot(1, 3, 2); plot_fit_gLV(ryobs[1],ry0[1],nsp,mu_h,M_h)\n",
    "plt.subplot(1, 3, 3); plot_fit_gLV(ryobs[2],ry0[2],nsp,mu_h,M_h)\n",
    "\n",
    "compare_params(mu_h, mu, M_h, M)\n",
    "\n",
    "# do some bootstrapping to help with interpretation of parameters\n",
    "# starred parameters are considered different to zero\n",
    "do_bootstrapping(X, F, nsp, a0, a1, nt, nboots=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc88f5d-7f4e-4383-a111-52d19c10ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five species, single time course including a perturbation\n",
    "nsp = 5\n",
    "times = np.arange(0, 5, 0.1)\n",
    "nt = len(times)\n",
    "print(f\"nt: {nt}\")\n",
    "\n",
    "yobs, y0, mu, M = sim_gLV_5_pert(times=times, y0=[10, 10, 10, 10, 10])\n",
    "    \n",
    "# add some gaussian noise\n",
    "yobs = yobs + np.random.normal(loc=0, scale=0.1, size=yobs.shape );\n",
    "plt.plot(times, yobs);\n",
    "plt.legend( ('1','2','3','4','5') );\n",
    "\n",
    "# time dependent perturbation\n",
    "u = (times >= 2) & (times < 3) \n",
    "u = u.astype(int)\n",
    "print(u)\n",
    "\n",
    "# linearise\n",
    "X, F = linearize_time_course_16S_u(yobs, times, u)\n",
    "print(f\"n: {nsp*F.shape[0]}, p: {nsp + nsp*nsp}\")\n",
    "\n",
    "# get the best lambda/alpha values on a grid via cross validation\n",
    "#a0, a1 = fit_alpha_Ridge2(X, F, nsp=nsp, n_a0=20, n_a1=20)\n",
    "\n",
    "# do final fit\n",
    "# a0, a1, a2 are constraint strengths on Mij, mu, epsilon\n",
    "a0 = 1\n",
    "a1 = 0.01\n",
    "a2 = 1\n",
    "mu_h, M_h, e_h = do_final_fit_Ridge2(X, F, nsp, a0, a1, a2)\n",
    "\n",
    "#plot_fit_gLV(yobs,y0,nsp,mu_h,M_h)\n",
    "\n",
    "#ridge_fit_pert(X, F, alphas, nsp):\n",
    "\n",
    "# this does the stem plots with orange crosses the actual parameters\n",
    "compare_params_p(mu_h, mu, M_h, M, e_h, e=np.array([0, -1, 0 , -1, 0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc81c1-b7ce-482f-a6a0-a645bc1ec2aa",
   "metadata": {},
   "source": [
    "## Simulate some time course data and metabolites\n",
    "This model assumes metabolite production is associated with abundance:  dS/dt = alpha X <br>\n",
    "Note that this model needs rethinking as it cannot handle negative productivities <br>\n",
    "In this simple example we don't need to infer the time course. We just linearize and estimate the elements of alpha with Lasso<br>\n",
    "Number of metabolites is 6 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df873518-15bf-48ea-8a59-8f6c6671bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some microbiota and metabolites\n",
    "nsp = 5\n",
    "nm = 6\n",
    "times = np.arange(0, 5, 0.1)\n",
    "nt = len(times)\n",
    "print(f\"nt: {nt}\")\n",
    "yobs, sobs, sy0, mu, M, alpha = sim_gMLV_6by5(times, y0=[10,10,10,10,10])\n",
    "\n",
    "# Add some noise\n",
    "yobs = yobs + np.random.normal(loc=0, scale=0.1, size=yobs.shape )\n",
    "#sobs = sobs + np.random.lognormal(0.01, 0.5, size=sobs.shape )\n",
    "sobs = sobs + np.random.normal(loc=0, scale=0.1, size=sobs.shape )\n",
    "\n",
    "plt.figure(figsize=(6.4*2,4.8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(times, yobs)\n",
    "plt.legend((0,1,2,3,4))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(times, sobs)\n",
    "plt.legend((0,1,2,3,4,5))\n",
    "\n",
    "# Linearize this problem\n",
    "X, S = linearise_time_course_metabolites(sobs, yobs, times)\n",
    "\n",
    "# get the best lambda/alpha value on a grid via cross validation\n",
    "a_min, a_se = fit_alpha_lasso(X, S, n_a=20)\n",
    "model = Lasso(fit_intercept=False, max_iter=10000, alpha=a_min)\n",
    "\n",
    "# just fit based on plot\n",
    "#gMLV_ML.plot_alpha_lasso(X, S, n_a=20)\n",
    "#model = Lasso(fit_intercept=False, max_iter=10000, alpha=2)\n",
    "\n",
    "model.fit(X, S)\n",
    "alpha_h = model.coef_\n",
    "#print(\"alpha_h:\",alpha_h.shape)\n",
    "\n",
    "print(\"\\ninferred params:\")\n",
    "print(\"A_hat/A:\")\n",
    "print(np.round(np.array(alpha_h),decimals=2))\n",
    "print(\"\\n\",np.array(alpha) )\n",
    "\n",
    "# plot the params\n",
    "# You can see Lasso does a pretty good job at picking out the metabolite, microbiota interactions\n",
    "plt.figure()\n",
    "plt.stem( np.arange(0,nm*nsp), np.array(alpha_h).flatten(), markerfmt=\"D\" )\n",
    "plt.stem( np.arange(0,nm*nsp), np.array(alpha).flatten(), markerfmt=\"X\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29a4de-868c-4108-b30d-14902dd4f903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
